"""
RSS –ø–∞—Ä—Å–µ—Ä.
"""

import feedparser
import requests
import asyncio
import time
from typing import List, Dict, Any
from bs4 import BeautifulSoup
from datetime import datetime
from core.domain.exceptions import RSSFeedError
from config.settings import settings


class RSSParser:
    """–ü–∞—Ä—Å–µ—Ä RSS –ª–µ–Ω—Ç."""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    async def parse_feed(self, feed_url: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç RSS –ª–µ–Ω—Ç—É."""
        try:
            # –ü–∞—Ä—Å–∏–º RSS –ª–µ–Ω—Ç—É
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:
                print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: RSS-–ª–µ–Ω—Ç–∞ {feed_url} –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏")
                print(f"üìã –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {feed.bozo_exception}")
            
            if not hasattr(feed, 'entries') or not feed.entries:
                print(f"‚ùå –õ–µ–Ω—Ç–∞ {feed_url} –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø–∏—Å–µ–π")
                return []
            
            articles = []
            feed_title = feed.feed.title if hasattr(feed.feed, 'title') else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫'
            
            for entry in feed.entries:
                try:
                    article_data = await self._parse_entry(entry, feed_title, feed_url)
                    if article_data:
                        articles.append(article_data)
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
            
            print(f"‚úÖ RSS –ª–µ–Ω—Ç–∞ {feed_title}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
            return articles
            
        except Exception as e:
            raise RSSFeedError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS –ª–µ–Ω—Ç—ã {feed_url}: {str(e)}")

    async def _parse_entry(self, entry, feed_title: str, feed_url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å RSS."""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        pub_date = None
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            pub_date = datetime(*entry.published_parsed[:6])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = self._extract_article_metadata(entry)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        full_content = await self._extract_full_content(entry.link)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        word_count, reading_time = self._calculate_reading_stats(full_content)
        
        return {
            'title': entry.title,
            'link': entry.link,
            'published': pub_date,
            'summary': entry.summary if hasattr(entry, 'summary') else '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è',
            'source': feed_title,
            'feed_url': feed_url,
            'content': full_content,
            'author': metadata['author'],
            'category': metadata['category'],
            'image_url': metadata['image_url'],
            'word_count': word_count,
            'reading_time': reading_time,
            'is_processed': True,
            'created_at': datetime.now()
        }

    def _extract_article_metadata(self, entry) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ RSS-–∑–∞–ø–∏—Å–∏."""
        metadata = {
            'author': None,
            'category': None,
            'image_url': None
        }
        
        # –ê–≤—Ç–æ—Ä
        if hasattr(entry, 'author'):
            metadata['author'] = entry.author
        elif hasattr(entry, 'author_detail') and hasattr(entry.author_detail, 'name'):
            metadata['author'] = entry.author_detail.name
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
        if hasattr(entry, 'tags') and entry.tags:
            categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]
            metadata['category'] = ', '.join(categories[:3])
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if hasattr(media, 'type') and 'image' in media.type:
                    metadata['image_url'] = media.url
                    break
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if not metadata['image_url'] and hasattr(entry, 'enclosures'):
            for enclosure in entry.enclosures:
                if hasattr(enclosure, 'type') and 'image' in enclosure.type:
                    metadata['image_url'] = enclosure.href
                    break
        
        return metadata

    async def _extract_full_content(self, article_url: str, max_retries: int = 3) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL."""
        for attempt in range(max_retries):
            try:
                response = self.session.get(article_url, timeout=settings.parsing.timeout)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for script in soup(["script", "style", "nav", "footer", "aside"]):
                    script.decompose()
                
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
                content_selectors = [
                    'article', '.article-content', '.post-content', '.entry-content',
                    '.content', '.main-content', '.story-content', '.news-content',
                    '[role="main"]', '.article-body', '.post-body'
                ]
                
                content = None
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(strip=True)
                        break
                
                if not content:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º body
                    body = soup.find('body')
                    if body:
                        content = body.get_text(strip=True)
                
                return content[:5000] if content else None  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                continue
        
        return None

    def _calculate_reading_stats(self, content: str) -> tuple[int, int]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á—Ç–µ–Ω–∏—è."""
        if not content:
            return 0, 0
        
        # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞)
        import re
        words = re.findall(r'\b\w+\b', content.lower())
        word_count = len(words)
        
        # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–æ 200 —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
        reading_time = max(1, word_count // settings.parsing.words_per_minute)
        
        return word_count, reading_time
