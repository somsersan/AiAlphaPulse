"""–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è LLM –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
import json
import time
from typing import Dict, Optional
import psycopg2

from .openrouter_client import OpenRouterClient
from .schema import (
    create_llm_news_table,
    get_unprocessed_clusters,
    get_cluster_representative_article,
    insert_llm_analyzed_news
)


class LLMNewsProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM"""
    
    def __init__(self, conn: psycopg2.extensions.connection, api_key: str = None, 
                 model: str = None):
        self.conn = conn
        self.llm_client = OpenRouterClient(api_key=api_key, model=model)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        create_llm_news_table(conn)
        
    def process_cluster(self, cluster_id: int) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä"""
        
        try:
            # –°–ù–ê–ß–ê–õ–ê –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–∏ —É–∂–µ —ç—Ç–æ—Ç –∫–ª–∞—Å—Ç–µ—Ä
            cursor = self.conn.cursor()
            cursor.execute("SELECT id FROM llm_analyzed_news WHERE id_cluster = %s", (cluster_id,))
            if cursor.fetchone():
                print(f"  ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω (–æ–±—Ä–∞–±–æ—Ç–∞–Ω –¥—Ä—É–≥–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º)")
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
            article = get_cluster_representative_article(self.conn, cluster_id)
            
            if not article:
                print(f"‚ö†Ô∏è  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π")
                return None
            
            print(f"üì∞ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä {cluster_id}: {article['title'][:60]}...")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ LLM
            analysis = self.llm_client.analyze_news(
                headline=article['title'],
                content=article['content'] or article['title']
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ URL –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cursor = self.conn.cursor()
            try:
                cursor.execute("""
                    SELECT url FROM cluster_members 
                    WHERE cluster_id = %s AND url IS NOT NULL
                """, (cluster_id,))
                urls = [row[0] for row in cursor.fetchall()]
            except psycopg2.Error as e:
                self.conn.rollback()
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è URL: {e}")
                urls = []
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            data = {
                'id_old': article['normalized_id'],
                'id_cluster': cluster_id,
                'headline': article['title'],
                'content': article['content'],
                'urls_json': json.dumps(urls, ensure_ascii=False),
                'published_time': article['published_at'],
                'ai_hotness': analysis['hotness'],
                'tickers_json': json.dumps(analysis['tickers'], ensure_ascii=False),
                'reasoning': analysis.get('reasoning', '')  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
            }
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –ë–î
            new_id = insert_llm_analyzed_news(self.conn, data)
            
            if not new_id:
                # –û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ - —ç—Ç–æ –Ω–µ –¥–æ–ª–∂–Ω–æ —Å–ª—É—á–∞—Ç—å—Å—è, —Ç.–∫. –º—ã –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                print(f"  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—Å—Ç–∞–≤–∏—Ç—å –≤ –ë–î (–≤–æ–∑–º–æ–∂–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç)")
                return None
            
            reasoning_short = analysis.get('reasoning', '')[:80] + '...' if len(analysis.get('reasoning', '')) > 80 else analysis.get('reasoning', '')
            print(f"  ‚úÖ ID={new_id} | üî• Hotness={analysis['hotness']:.3f} | üìä Tickers={analysis['tickers']}")
            if reasoning_short:
                print(f"     üí° {reasoning_short}")
            return data
                
        except psycopg2.Error as e:
            # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –ø—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ –ë–î
            self.conn.rollback()
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ë–î –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {e}")
            return None
        except Exception as e:
            print(f"  ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {e}")
            return None
    
    def process_batch(self, limit: int = 10, delay: float = 1.0) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞–∫–µ—Ç –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        
        stats = {
            'processed': 0,
            'skipped': 0,
            'errors': 0
        }
        
        total_requested = limit
        batch_size = 50  # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏ –¥–ª—è —Å–≤–µ–∂–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        
        print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É (—Ü–µ–ª—å: {total_requested} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)...\n")
        
        while stats['processed'] < total_requested:
            # –°–∫–æ–ª—å–∫–æ –µ—â–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
            remaining = total_requested - stats['processed']
            fetch_limit = min(remaining, batch_size)
            
            # –ü–æ–ª—É—á–∞–µ–º –°–í–ï–ñ–ò–ô —Å–ø–∏—Å–æ–∫ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            clusters = get_unprocessed_clusters(self.conn, limit=fetch_limit)
            
            if not clusters:
                print(f"\n‚úÖ –ë–æ–ª—å—à–µ –Ω–µ—Ç –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤!")
                break
            
            print(f"üì¶ –ë–∞—Ç—á: –Ω–∞–π–¥–µ–Ω–æ {len(clusters)} –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            for cluster in clusters:
                # –¢–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                current = stats['processed'] + stats['skipped'] + stats['errors'] + 1
                
                try:
                    print(f"[{current}/{total_requested}] ", end="")
                    
                    result = self.process_cluster(cluster['cluster_id'])
                    
                    if result:
                        stats['processed'] += 1
                    elif result is None:
                        stats['skipped'] += 1
                    else:
                        stats['errors'] += 1
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API
                    time.sleep(delay)
                    
                    # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• (–Ω–µ —Å—á–∏—Ç–∞—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ)
                    if stats['processed'] >= total_requested:
                        break
                        
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
                    stats['errors'] += 1
                    continue
        
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò")
        print(f"{'='*60}")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {stats['processed']}")
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã): {stats['skipped']}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}")
        total_attempts = stats['processed'] + stats['errors']
        if total_attempts > 0:
            print(f"üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {stats['processed']*100/total_attempts:.1f}%")
        
        return stats
    
    def get_top_hot_news(self, limit: int = 10, min_hotness: float = 0.7):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø —Å–∞–º—ã—Ö –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        
        query = """
        SELECT 
            id,
            headline,
            ai_hotness,
            tickers_json,
            published_time,
            urls_json
        FROM llm_analyzed_news
        WHERE ai_hotness >= %s
        ORDER BY ai_hotness DESC, published_time DESC
        LIMIT %s
        """
        
        cursor = self.conn.cursor()
        cursor.execute(query, (min_hotness, limit))
        
        columns = [desc[0] for desc in cursor.description]
        results = []
        for row in cursor.fetchall():
            data = dict(zip(columns, row))
            # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è
            data['tickers'] = json.loads(data['tickers_json']) if data['tickers_json'] else []
            data['urls'] = json.loads(data['urls_json']) if data['urls_json'] else []
            results.append(data)
        
        return results

