"""–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è LLM –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
import json
import time
from typing import Dict, Optional
import psycopg2

from .openrouter_client import OpenRouterClient
from .schema import (
    create_llm_news_table,
    get_unprocessed_clusters,
    get_cluster_representative_article,
    insert_llm_analyzed_news
)


class LLMNewsProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM"""
    
    def __init__(self, conn: psycopg2.extensions.connection, api_key: str = None, 
                 model: str = None):
        self.conn = conn
        self.llm_client = OpenRouterClient(api_key=api_key, model=model)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        create_llm_news_table(conn)
        
    def process_cluster(self, cluster_id: int) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä"""
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
        article = get_cluster_representative_article(self.conn, cluster_id)
        
        if not article:
            print(f"‚ö†Ô∏è  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π")
            return None
        
        print(f"üì∞ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä {cluster_id}: {article['title'][:60]}...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ LLM
        analysis = self.llm_client.analyze_news(
            headline=article['title'],
            content=article['content'] or article['title']
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ URL –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT url FROM cluster_members 
            WHERE cluster_id = %s AND url IS NOT NULL
        """, (cluster_id,))
        urls = [row[0] for row in cursor.fetchall()]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        data = {
            'id_old': article['normalized_id'],
            'id_cluster': cluster_id,
            'headline': article['title'],
            'content': article['content'],
            'urls_json': json.dumps(urls, ensure_ascii=False),
            'published_time': article['published_at'],
            'ai_hotness': analysis['hotness'],
            'tickers_json': json.dumps(analysis['tickers'], ensure_ascii=False)
        }
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –ë–î
        new_id = insert_llm_analyzed_news(self.conn, data)
        
        if new_id:
            print(f"  ‚úÖ ID={new_id} | üî• Hotness={analysis['hotness']:.3f} | üìä Tickers={analysis['tickers']}")
            return data
        else:
            print(f"  ‚ö†Ô∏è  –ö–ª–∞—Å—Ç–µ—Ä —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            return None
    
    def process_batch(self, limit: int = 10, delay: float = 1.0) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞–∫–µ—Ç –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        
        print(f"\nüîç –ò—â–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–ª–∏–º–∏—Ç: {limit})...")
        
        clusters = get_unprocessed_clusters(self.conn, limit=limit)
        
        if not clusters:
            print("‚úÖ –í—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            return {
                'processed': 0,
                'skipped': 0,
                'errors': 0
            }
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}\n")
        
        stats = {
            'processed': 0,
            'skipped': 0,
            'errors': 0
        }
        
        for i, cluster in enumerate(clusters, 1):
            try:
                print(f"[{i}/{len(clusters)}] ", end="")
                
                result = self.process_cluster(cluster['cluster_id'])
                
                if result:
                    stats['processed'] += 1
                else:
                    stats['skipped'] += 1
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API
                if i < len(clusters):
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
                stats['errors'] += 1
                continue
        
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò")
        print(f"{'='*60}")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}")
        
        return stats
    
    def get_top_hot_news(self, limit: int = 10, min_hotness: float = 0.7):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø —Å–∞–º—ã—Ö –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        
        query = """
        SELECT 
            id,
            headline,
            ai_hotness,
            tickers_json,
            published_time,
            urls_json
        FROM llm_analyzed_news
        WHERE ai_hotness >= %s
        ORDER BY ai_hotness DESC, published_time DESC
        LIMIT %s
        """
        
        cursor = self.conn.cursor()
        cursor.execute(query, (min_hotness, limit))
        
        columns = [desc[0] for desc in cursor.description]
        results = []
        for row in cursor.fetchall():
            data = dict(zip(columns, row))
            # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è
            data['tickers'] = json.loads(data['tickers_json']) if data['tickers_json'] else []
            data['urls'] = json.loads(data['urls_json']) if data['urls_json'] else []
            results.append(data)
        
        return results

