"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ JSON
"""
import json
import sqlite3
import argparse
from datetime import datetime
from pathlib import Path


def export_normalized_to_json(db_path: str, output_path: str, limit: int = None, min_quality: float = 0.0, language: str = None):
    """–≠–∫—Å–ø–æ—Ä—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ JSON"""
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    query = """
    SELECT 
        original_id,
        title,
        content,
        link,
        source,
        published_at,
        language_code,
        entities_json,
        quality_score,
        word_count,
        created_at
    FROM normalized_articles
    WHERE quality_score >= ?
    """
    
    params = [min_quality]
    
    if language:
        query += " AND language_code = ?"
        params.append(language)
    
    query += " ORDER BY quality_score DESC, published_at DESC"
    
    if limit:
        query += " LIMIT ?"
        params.append(limit)
    
    cursor = conn.execute(query, params)
    rows = cursor.fetchall()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    articles = []
    for row in rows:
        article = dict(row)
        
        # –ü–∞—Ä—Å–∏–º JSON —Å—Ç—Ä–æ–∫—É —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        try:
            article['entities'] = json.loads(article['entities_json'])
        except:
            article['entities'] = []
        
        # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –ø–æ–ª–µ entities_json
        del article['entities_json']
        
        articles.append(article)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∞
    export_data = {
        'metadata': {
            'export_date': datetime.now().isoformat(),
            'total_articles': len(articles),
            'min_quality_filter': min_quality,
            'language_filter': language,
            'limit_applied': limit,
            'database_path': db_path
        },
        'articles': articles
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
    
    conn.close()
    
    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –≤ {output_path}")
    print(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
    print(f"   - –î–∞—Ç–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {export_data['metadata']['export_date']}")
    print(f"   - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞: {min_quality}")
    print(f"   - –§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É: {language or '–Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω'}")
    print(f"   - –õ–∏–º–∏—Ç: {limit or '–Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω'}")
    
    return len(articles)


def export_all_articles(db_path: str, output_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    return export_normalized_to_json(db_path, output_path)


def export_high_quality_articles(db_path: str, output_path: str, min_quality: float = 0.8):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    return export_normalized_to_json(db_path, output_path, min_quality=min_quality)


def export_by_language(db_path: str, output_path: str, language: str, limit: int = None):
    """–≠–∫—Å–ø–æ—Ä—Ç —Å—Ç–∞—Ç–µ–π –ø–æ —è–∑—ã–∫—É"""
    return export_normalized_to_json(db_path, output_path, language=language, limit=limit)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–≠–∫—Å–ø–æ—Ä—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ JSON')
    parser.add_argument('--db', default='data/rss_articles.db', help='–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--output', default='normalized_articles.json', help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É')
    parser.add_argument('--limit', type=int, help='–õ–∏–º–∏—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞')
    parser.add_argument('--min-quality', type=float, default=0.0, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞')
    parser.add_argument('--language', help='–§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É (ru, en, etc.)')
    parser.add_argument('--high-quality', action='store_true', help='–≠–∫—Å–ø–æ—Ä—Ç —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (>=0.8)')
    parser.add_argument('--all', action='store_true', help='–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π')
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    if args.output == 'normalized_articles.json':
        if args.high_quality:
            args.output = 'high_quality_articles.json'
        elif args.language:
            args.output = f'articles_{args.language}.json'
        elif args.all:
            args.output = 'all_normalized_articles.json'
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    if args.high_quality:
        args.min_quality = 0.8
    
    try:
        count = export_normalized_to_json(
            args.db, 
            args.output, 
            args.limit, 
            args.min_quality, 
            args.language
        )
        print(f"\nüéâ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} —Å—Ç–∞—Ç–µ–π.")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
