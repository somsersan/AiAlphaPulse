"""
–ú–æ–¥—É–ª—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
"""
import re
import html
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
import langdetect
from langdetect import detect
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import unicodedata


class NewsNormalizer:
    """–ö–ª–∞—Å—Å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.spam_patterns = [
            r'—Ä–µ–∫–ª–∞–º–∞',
            r'—Å–ø–æ–Ω—Å–æ—Ä',
            r'–ø–∞—Ä—Ç–Ω–µ—Ä',
            r'–∫—É–ø–∏—Ç—å\s+—Å–µ–π—á–∞—Å',
            r'—Å–∫–∏–¥–∫–∞\s+\d+%',
            r'—Ç–æ–ª—å–∫–æ\s+—Å–µ–≥–æ–¥–Ω—è',
            r'–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ\s+–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ',
            r'–∫–ª–∏–∫–Ω–∏—Ç–µ\s+–∑–¥–µ—Å—å',
            r'–ø–æ–¥—Ä–æ–±–Ω–µ–µ\s+–Ω–∞\s+—Å–∞–π—Ç–µ',
            r'–ø–µ—Ä–µ–π—Ç–∏\s+–ø–æ\s+—Å—Å—ã–ª–∫–µ'
        ]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK –¥–∞–Ω–Ω—ã—Ö
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')
        
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            nltk.download('wordnet')
    
    def clean_html(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML entities"""
        if not text:
            return ""
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML entities
        text = html.unescape(text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤
        soup = BeautifulSoup(text, 'html.parser')
        text = soup.get_text()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text.strip()) < 10:
            return 'unknown'
        
        try:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞
            sample_text = text[:1000]
            language = detect(sample_text)
            return language
        except:
            return 'unknown'
    
    def is_spam(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""
        if not text:
            return True
        
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(text.strip()) < 50:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–∞–º-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in self.spam_patterns:
            if re.search(pattern, text_lower):
                return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —ç–º–æ–¥–∑–∏
        emoji_count = len(re.findall(r'[üòÄ-üôèüåÄ-üóø]', text))
        if emoji_count > len(text) * 0.1:  # –ë–æ–ª–µ–µ 10% —ç–º–æ–¥–∑–∏
            return True
        
        return False
    
    def normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∫–∞, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è"""
        if not text:
            return ""
        
        # –û—á–∏—Å—Ç–∫–∞ HTML
        text = self.clean_html(text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode
        text = unicodedata.normalize('NFKD', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_entities(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–∫–æ–º–ø–∞–Ω–∏–∏, —Ç–∏–∫–µ—Ä—ã, —Å—Ç—Ä–∞–Ω—ã)"""
        entities = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–∏–∫–µ—Ä–æ–≤ (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã, 2-5 —Å–∏–º–≤–æ–ª–æ–≤)
        tickers = re.findall(r'\b[A-Z]{2,5}\b', text)
        entities.extend(tickers)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π (—Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã)
        companies = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        entities.extend(companies)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        entities = list(set(entities))
        entities.sort()
        
        return entities[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def calculate_quality_score(self, article: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç—å–∏ (0-1)"""
        score = 0.0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if not article.get('content') or len(article['content'].strip()) < 100:
            return 0.0
        
        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_length = len(article['content'])
        if content_length > 500:
            score += 0.3
        elif content_length > 200:
            score += 0.2
        
        # –ù–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if article.get('title') and len(article['title'].strip()) > 10:
            score += 0.2
        
        # –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–∫–∏
        if article.get('link'):
            score += 0.1
        
        # –ù–∞–ª–∏—á–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if article.get('source'):
            score += 0.1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º
        if not self.is_spam(article.get('content', '')):
            score += 0.3
        else:
            score *= 0.3  # –°–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ–º –æ—Ü–µ–Ω–∫—É –∑–∞ —Å–ø–∞–º
        
        return min(score, 1.0)
    
    def normalize_article(self, article: Dict) -> Optional[Dict]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º
        if self.is_spam(article.get('content', '')):
            return None
        
        # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        normalized_content = self.normalize_text(article.get('content', ''))
        normalized_title = self.normalize_text(article.get('title', ''))
        
        if not normalized_content or len(normalized_content.strip()) < 50:
            return None
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
        language = self.detect_language(normalized_content)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entities = self.extract_entities(normalized_content)
        
        # –†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = self.calculate_quality_score(article)
        
        if quality_score < 0.3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞
            return None
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
        normalized_article = {
            'original_id': article.get('id'),
            'title': normalized_title,
            'content': normalized_content,
            'link': article.get('link'),
            'source': article.get('source'),
            'published_at': article.get('published'),
            'language_code': language,
            'entities': entities,
            'quality_score': quality_score,
            'word_count': len(normalized_content.split()),
            'is_processed': True
        }
        
        return normalized_article
