"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
"""
import json
import sqlite3
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict

from .normalizer import NewsNormalizer
from .database_schema import (
    create_normalized_articles_table,
    create_processing_log_table,
    get_processed_articles,
    get_max_processed_id,
    get_unprocessed_articles,
    insert_normalized_article,
    log_processing_batch,
    get_processing_stats
)


class ArticleProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç–µ–π"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.normalizer = NewsNormalizer()
        self.conn = None
    
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        self.conn = sqlite3.connect(self.db_path)
        self.conn.row_factory = sqlite3.Row
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        create_normalized_articles_table(self.conn)
        create_processing_log_table(self.conn)
    
    def close_db(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π"""
        if self.conn:
            self.conn.close()
    
    def load_articles_from_json(self, json_path: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        with open(json_path, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        return articles
    
    def load_articles_from_db(self, limit: int = None) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        query = """
        SELECT id, title, link, source, published, is_processed, summary, content
        FROM articles
        ORDER BY published DESC
        """
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor = self.conn.execute(query)
        rows = cursor.fetchall()
        return [dict(row) for row in rows]
    
    def load_unprocessed_articles(self, limit: int = None) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)"""
        return get_unprocessed_articles(self.conn, limit)
    
    def get_processing_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        max_processed_id = get_max_processed_id(self.conn)
        processed_count = len(get_processed_articles(self.conn))
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
        cursor = self.conn.execute("SELECT COUNT(*) FROM articles")
        total_articles = cursor.fetchone()[0]
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ID –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
        cursor = self.conn.execute("SELECT MAX(id) FROM articles")
        max_original_id = cursor.fetchone()[0] or 0
        
        return {
            'max_processed_id': max_processed_id,
            'processed_count': processed_count,
            'total_articles': total_articles,
            'max_original_id': max_original_id,
            'unprocessed_count': max_original_id - max_processed_id,
            'is_up_to_date': max_processed_id >= max_original_id
        }
    
    def process_articles_batch(self, articles: List[Dict], batch_size: int = 100) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ —Å—Ç–∞—Ç–µ–π"""
        start_time = time.time()
        batch_id = str(uuid.uuid4())
        
        # –ü–æ–ª—É—á–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        processed_ids = get_processed_articles(self.conn)
        
        stats = {
            'batch_id': batch_id,
            'total_articles': len(articles),
            'processed_articles': 0,
            'filtered_articles': 0,
            'error_count': 0,
            'processing_time_seconds': 0
        }
        
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–∞–∫–µ—Ç–∞ {batch_id}")
        print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_ids)}")
        
        for i, article in enumerate(articles):
            try:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                if article['id'] in processed_ids:
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏
                normalized_article = self.normalizer.normalize_article(article)
                
                if normalized_article:
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É
                    insert_normalized_article(self.conn, normalized_article)
                    stats['processed_articles'] += 1
                    
                    if stats['processed_articles'] % 10 == 0:
                        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed_articles']}")
                else:
                    stats['filtered_articles'] += 1
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ {article.get('id', 'unknown')}: {e}")
                stats['error_count'] += 1
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        end_time = time.time()
        stats['processing_time_seconds'] = round(end_time - start_time, 2)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        log_processing_batch(self.conn, stats)
        
        print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {stats['processing_time_seconds']} —Å–µ–∫—É–Ω–¥")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed_articles']}")
        print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats['filtered_articles']}")
        print(f"–û—à–∏–±–æ–∫: {stats['error_count']}")
        
        return stats
    
    def process_from_json(self, json_path: str, batch_size: int = 100):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ {json_path}")
        articles = self.load_articles_from_json(json_path)
        
        self.connect_db()
        try:
            self.process_articles_batch(articles, batch_size)
        finally:
            self.close_db()
    
    def process_from_db(self, limit: int = None, batch_size: int = 100):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        self.connect_db()
        try:
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–ª–∏–º–∏—Ç: {limit})")
            articles = self.load_articles_from_db(limit)
            self.process_articles_batch(articles, batch_size)
        finally:
            self.close_db()
    
    def process_unprocessed_articles(self, limit: int = None, batch_size: int = 100):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)"""
        self.connect_db()
        try:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
            status = self.get_processing_status()
            print(f"üìä –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {status['processed_count']}")
            print(f"   - –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {status['total_articles']}")
            print(f"   - –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {status['unprocessed_count']}")
            print(f"   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π ID: {status['max_processed_id']}")
            
            if status['is_up_to_date']:
                print("‚úÖ –í—Å–µ —Å—Ç–∞—Ç—å–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                return
            
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (–ª–∏–º–∏—Ç: {limit})")
            articles = self.load_unprocessed_articles(limit)
            
            if not articles:
                print("‚ÑπÔ∏è  –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return
            
            print(f"üìù –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            self.process_articles_batch(articles, batch_size)
            
        finally:
            self.close_db()
    
    def show_processing_status(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.connect_db()
        try:
            status = self.get_processing_status()
            
            print("=== –°–¢–ê–¢–£–° –û–ë–†–ê–ë–û–¢–ö–ò ===")
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {status['processed_count']}")
            print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {status['total_articles']}")
            print(f"–ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {status['unprocessed_count']}")
            print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π ID: {status['max_processed_id']}")
            print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π ID: {status['max_original_id']}")
            print(f"–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: {'‚úÖ –ê–∫—Ç—É–∞–ª—å–Ω–æ' if status['is_up_to_date'] else '‚ùå –ï—Å—Ç—å –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏'}")
            
        finally:
            self.close_db()
    
    def show_stats(self):
        """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.connect_db()
        try:
            stats = get_processing_stats(self.conn)
            
            print("\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò ===")
            print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ: {stats['total_original_articles']}")
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {stats['total_processed_articles']}")
            print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {(stats['total_processed_articles'] / stats['total_original_articles'] * 100):.1f}%")
            print(f"–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞: {stats['average_quality_score']}")
            
            print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º:")
            for lang, count in stats['language_distribution'].items():
                print(f"  {lang}: {count}")
            
            print("\n–¢–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
            for source, count in stats['top_sources'].items():
                print(f"  {source}: {count}")
                
        finally:
            self.close_db()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π')
    parser.add_argument('--json', help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Å—Ç–∞—Ç—å—è–º–∏')
    parser.add_argument('--db', help='–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö', default='data/rss_articles.db')
    parser.add_argument('--limit', type=int, help='–õ–∏–º–∏—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞')
    parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É')
    parser.add_argument('--status', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--unprocessed', action='store_true', help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏')
    
    args = parser.parse_args()
    
    processor = ArticleProcessor(args.db)
    
    if args.stats:
        processor.show_stats()
    elif args.status:
        processor.show_processing_status()
    elif args.json:
        processor.process_from_json(args.json, args.batch_size)
    elif args.unprocessed:
        processor.process_unprocessed_articles(args.limit, args.batch_size)
    else:
        processor.process_from_db(args.limit, args.batch_size)


if __name__ == "__main__":
    main()
