import feedparser
import requests
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean
from sqlalchemy.orm import sessionmaker, declarative_base
from datetime import datetime
from dotenv import load_dotenv
import re
import time
import os


    # # Investopedia (–í—Å–µ —Å—Ç–∞—Ç—å–∏)
    # 'https://news.google.com/rss?hl=ru&gl=RU&ceid=RU:ru',
    # 'https://www.investopedia.com/rss-feed-4790074',
RSS_URLS = [
    # ----------------------------------------------------
    # –†–û–°–°–ò–ô–°–ö–ò–ï –§–ò–ù–ê–ù–°–û–í–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò
    # ----------------------------------------------------
    "https://smart-lab.ru/news/rss/",
    "https://smart-lab.ru/forum/rss/",
    'https://tass.ru/rss/v2.xml',  # –¢–ê–°–°
    'http://static.feed.rbc.ru/rbc/logical/footer/news.rss',  # –†–ë–ö
    'https://www.vedomosti.ru/rss/news',  # –í–µ–¥–æ–º–æ—Å—Ç–∏
    'https://www.kommersant.ru/RSS/news.xml',  # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä
    'https://www.kommersant.ru/RSS/finance.xml',  # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä –§–∏–Ω–∞–Ω—Å—ã
    'https://lenta.ru/rss/news',  # –õ–µ–Ω—Ç–∞.—Ä—É
    
    # ----------------------------------------------------
    # –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–ò–ù–ê–ù–°–û–í–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò
    # ----------------------------------------------------
    'https://feeds.bloomberg.com/markets/news.rss',  # Bloomberg Markets
    'https://feeds.bloomberg.com/business/news.rss',  # Bloomberg Business
    'https://www.cnbc.com/id/100003114/device/rss/rss.html',  # CNBC
    'https://www.businessinsider.com/rss',  # Business Insider
    'https://www.ft.com/?format=rss',  # Financial Times
    'https://fortune.com/feed',  # Fortune
    'https://www.investing.com/rss/news.rss',  # Investing.com
    'https://finance.yahoo.com/news/rssindex',  # Yahoo Finance
    'https://financialpost.com/feed',  # Financial Post
]

# PostgreSQL –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
load_dotenv()
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://admin:04102025@130.193.55.244:5432/alphapulse')

# --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ë–î (SQLAlchemy) ---
Base = declarative_base()

class Article(Base):
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True)
    title = Column(String, nullable=False, unique=True)
    link = Column(String, nullable=False)
    published = Column(DateTime)
    summary = Column(Text)
    source = Column(String)
    feed_url = Column(String)
    content = Column(Text)  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    author = Column(String)  # –ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏
    category = Column(String)  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
    image_url = Column(String)  # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    word_count = Column(Integer)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    reading_time = Column(Integer)  # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
    is_processed = Column(Boolean, default=False)  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ª–∏ —Å—Ç–∞—Ç—å—è
    created_at = Column(DateTime, default=datetime.now)  # –ö–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ë–î

    def __repr__(self):
        return f"<Article(title='{self.title[:30]}...', source='{self.source}')>"

# --- 3. –§—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---

def extract_full_content(article_url, max_retries=3):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL."""
    for attempt in range(max_retries):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for script in soup(["script", "style", "nav", "footer", "aside"]):
                script.decompose()
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            content_selectors = [
                'article', '.article-content', '.post-content', '.entry-content',
                '.content', '.main-content', '.story-content', '.news-content',
                '[role="main"]', '.article-body', '.post-body'
            ]
            
            content = None
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            if not content:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º body
                body = soup.find('body')
                if body:
                    content = body.get_text(strip=True)
            
            return content[:5000] if content else None  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
            continue
    
    return None

def extract_article_metadata(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ RSS-–∑–∞–ø–∏—Å–∏."""
    metadata = {
        'author': None,
        'category': None,
        'image_url': None
    }
    
    # –ê–≤—Ç–æ—Ä
    if hasattr(entry, 'author'):
        metadata['author'] = entry.author
    elif hasattr(entry, 'author_detail') and hasattr(entry.author_detail, 'name'):
        metadata['author'] = entry.author_detail.name
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
    if hasattr(entry, 'tags') and entry.tags:
        categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]
        metadata['category'] = ', '.join(categories[:3])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ç–µ–≥–∞
    
    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if hasattr(media, 'type') and 'image' in media.type:
                metadata['image_url'] = media.url
                break
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    if not metadata['image_url'] and hasattr(entry, 'enclosures'):
        for enclosure in entry.enclosures:
            if hasattr(enclosure, 'type') and 'image' in enclosure.type:
                metadata['image_url'] = enclosure.href
                break
    
    return metadata

def calculate_reading_stats(content):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á—Ç–µ–Ω–∏—è."""
    if not content:
        return 0, 0
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞)
    words = re.findall(r'\b\w+\b', content.lower())
    word_count = len(words)
    
    # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–æ 200 —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
    reading_time = max(1, word_count // 200)
    
    return word_count, reading_time

def setup_database():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∏ —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
    engine = create_engine(DATABASE_URL)
    Base.metadata.create_all(engine) 
    Session = sessionmaker(bind=engine)
    return Session()

def parse_and_save_rss_custom(session, urls=None):
    """–ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL, –ø–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é –ª–µ–Ω—Ç—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –ë–î."""
    if urls is None:
        urls = RSS_URLS
    global_new_count = 0
    
    print(f"üõ†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(urls)} RSS-–ª–µ–Ω—Ç...")
    
    for url in urls:
        try:
            print(f"üîç –ü–∞—Ä—Å–∏–º –ª–µ–Ω—Ç—É {url}")
            feed = feedparser.parse(url)
            
            if feed.bozo:
                print(f"   ‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: RSS-–ª–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏")
                print(f"   üìã –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {feed.bozo_exception}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–ø–∏—Å–∏ –≤ –ª–µ–Ω—Ç–µ
            if not hasattr(feed, 'entries') or not feed.entries:
                print(f"   ‚ùå –õ–µ–Ω—Ç–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø–∏—Å–µ–π")
                continue
            
            new_count = 0
            feed_title = feed.feed.title if hasattr(feed.feed, 'title') else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫'
            print(f"   üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {feed_title}")
            
            for i, entry in enumerate(feed.entries):
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è
                    exists = session.query(Article).filter_by(title=entry.title).first()
                    if exists:
                        continue
                    
                    print(f"   üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i+1}/{len(feed.entries)}: {entry.title[:50]}...")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata = extract_article_metadata(entry)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
                    print(f"      üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
                    full_content = extract_full_content(entry.link)
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    word_count, reading_time = calculate_reading_stats(full_content)
                    
                    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    new_article = Article(
                        title=entry.title,
                        link=entry.link,
                        published=pub_date,
                        summary=entry.summary if hasattr(entry, 'summary') else '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è',
                        source=feed_title,
                        feed_url=url,
                        content=full_content,
                        author=metadata['author'],
                        category=metadata['category'],
                        image_url=metadata['image_url'],
                        word_count=word_count,
                        reading_time=reading_time,
                        is_processed=True
                    )
                    
                    session.add(new_article)
                    new_count += 1
                    global_new_count += 1
                    
                    print(f"      ‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ (—Å–ª–æ–≤: {word_count}, –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {reading_time} –º–∏–Ω)")
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
            
            print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(feed.entries)}, –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {new_count}")
            
        except Exception as e:
            print(f"   - üîß –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –ª–µ–Ω—Ç—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
            continue

    try:
        session.commit()
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        print(f"   –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {global_new_count}")
        return global_new_count
    except Exception as e:
        session.rollback()
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–∫—Å–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏: {e}")
        return 0
    finally:
        pass  # –ù–µ –∑–∞–∫—Ä—ã–≤–∞–µ–º session, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∏–∑–≤–Ω–µ

def parse_and_save_rss():
    """–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    session = setup_database()
    try:
        return parse_and_save_rss_custom(session)
    finally:
        session.close()

# --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–†–û–í–ï–†–ö–ò ---
def check_articles_custom(session, limit=10):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 'limit' —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î."""
    # –ó–∞–ø—Ä–æ—Å –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ ID (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–Ω–∏–∑—É)
    articles = session.query(Article).order_by(Article.id.desc()).limit(limit).all()
    
    print(f"\n--- –ü–æ—Å–ª–µ–¥–Ω–∏–µ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ---")
    if not articles:
        print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞.")
        return []
    
    result = []
    for article in articles:
        article_data = {
            'id': article.id,
            'source': article.source,
            'title': article.title,
            'author': article.author or '–ù–µ —É–∫–∞–∑–∞–Ω',
            'category': article.category or '–ù–µ —É–∫–∞–∑–∞–Ω–∞',
            'published': article.published.strftime('%Y-%m-%d %H:%M:%S') if article.published else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö',
            'created_at': article.created_at.strftime('%Y-%m-%d %H:%M:%S') if article.created_at else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö',
            'word_count': article.word_count or 0,
            'reading_time': article.reading_time or 0,
            'is_processed': article.is_processed,
            'link': article.link,
            'image_url': article.image_url,
            'summary': article.summary,
            'content': article.content
        }
        result.append(article_data)
    
    return result

def check_articles(limit=10):
    """–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    session = setup_database()
    try:
        return check_articles_custom(session, limit)
    finally:
        session.close()

def get_articles_stats_custom(session):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—å—è–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    
    total_articles = session.query(Article).count()
    processed_articles = session.query(Article).filter(Article.is_processed == True).count()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    sources = session.query(Article.source, session.query(Article).filter(Article.source == Article.source).count()).group_by(Article.source).all()
    
    # –°—Ä–µ–¥–Ω—è—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    avg_words = session.query(Article.word_count).filter(Article.word_count.isnot(None)).all()
    avg_words = sum([w[0] for w in avg_words]) / len(avg_words) if avg_words else 0
    
    stats = {
        'total_articles': total_articles,
        'processed_articles': processed_articles,
        'avg_words': round(avg_words, 0),
        'sources': [{'source': source, 'count': count} for source, count in sources]
    }
    
    return stats

def get_articles_stats():
    """–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    session = setup_database()
    try:
        return get_articles_stats_custom(session)
    finally:
        session.close()


class RSSParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS –ª–µ–Ω—Ç –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, database_url: str = None):
        self.database_url = database_url or DATABASE_URL
        self.session = None
    
    def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        engine = create_engine(self.database_url)
        Base.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        self.session = Session()
        return self.session
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.session:
            self.session.close()
    
    def parse_and_save(self, urls: list = None):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π"""
        if not self.session:
            self.connect()
        
        urls = urls or RSS_URLS
        return parse_and_save_rss_custom(self.session, urls)
    
    def get_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.session:
            self.connect()
        
        return get_articles_stats_custom(self.session)
    
    def check_articles(self, limit: int = 10):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π"""
        if not self.session:
            self.connect()
        
        return check_articles_custom(self.session, limit)
