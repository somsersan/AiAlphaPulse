import json
from dataclasses import dataclass, field
from datetime import datetime
import re
import time
from typing import Dict, List, Optional
from urllib.parse import urljoin

import feedparser
import requests
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean
from sqlalchemy.orm import sessionmaker, declarative_base

from .rss_parser import (
    calculate_reading_stats,
    extract_article_metadata,
    extract_full_content,
)


    # # Investopedia (–í—Å–µ —Å—Ç–∞—Ç—å–∏)
    # 'https://news.google.com/rss?hl=ru&gl=RU&ceid=RU:ru',
    # 'https://www.investopedia.com/rss-feed-4790074',
RSS_URLS = [
    # ----------------------------------------------------
    # –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–ò–ù–ê–ù–°–û–í–´–ï –ò –ë–ò–ó–ù–ï–°-–ò–ó–î–ê–ù–ò–Ø
    # ----------------------------------------------------
    # 'https://lenta.ru/rss/news',
    # 'https://habr.com/ru/rss/hubs/all/'
    
    # # Bloomberg
    # 'https://feeds.bloomberg.com/markets/news.rss',
    # 'https://feeds.bloomberg.com/business/news.rss',
        
    # # CNBC (Top News)
    # 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
        
        
    # # Business Insider (Top News)
    # 'https://www.businessinsider.com/rss',


    # "https://smart-lab.ru/news/rss/",
    # "https://smart-lab.ru/forum/rss/",
    
    # # ----------------------------------------------------
    # # –†–û–°–°–ò–ô–°–ö–ò–ï / –†–£–°–°–ö–û–Ø–ó–´–ß–ù–´–ï
    # # ----------------------------------------------------
    
    # # –†–ë–ö (–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏)
    # 'http://static.feed.rbc.ru/rbc/logical/footer/news.rss',
    
    # # –í–µ–¥–æ–º–æ—Å—Ç–∏ (–ì–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏)
    # 'https://www.vedomosti.ru/rss/news',
    
    # # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä (–§–∏–Ω–∞–Ω—Å—ã / –†—ã–Ω–∫–∏)
    # 'https://www.kommersant.ru/RSS/finance.xml',
    
    # –†–∞–±–æ—á–∏–µ RSS-–ª–µ–Ω—Ç—ã
    # 'https://tass.ru/rss/v2.xml',  # –¢–ê–°–° - —Ä–∞–±–æ—Ç–∞–µ—Ç
    
    # ----------------------------------------------------
    # –≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ò–ï RSS-–õ–ï–ù–¢–´ (–ü–†–û–í–ï–†–ï–ù–´)
    # ----------------------------------------------------
    # 'https://www.ft.com/?format=rss',  # Financial Times - —Ä–∞–±–æ—Ç–∞–µ—Ç
    # 'https://fortune.com/feed',  # Fortune - —Ä–∞–±–æ—Ç–∞–µ—Ç
    # 'https://www.investing.com/rss/news.rss',  # Investing.com - —Ä–∞–±–æ—Ç–∞–µ—Ç
    # 'https://finance.yahoo.com/news/rssindex',  # Yahoo Finance - —Ä–∞–±–æ—Ç–∞–µ—Ç
    # 'https://financialpost.com/feed',  # Financial Post - —Ä–∞–±–æ—Ç–∞–µ—Ç
    
    # ----------------------------------------------------
    # –†–û–°–°–ò–ô–°–ö–ò–ï –≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ò–ï RSS-–õ–ï–ù–¢–´ (–ü–†–û–í–ï–†–ï–ù–´)
    # ----------------------------------------------------
    # 'https://www.kommersant.ru/RSS/news.xml',  # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä - —Ä–∞–±–æ—Ç–∞–µ—Ç
]

# –ò–º—è —Ñ–∞–π–ª–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite
DATABASE_FILE = 'rss_articles2.db'
DATABASE_URL = f"sqlite:///{DATABASE_FILE}"

# --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ë–î (SQLAlchemy) ---
Base = declarative_base()

class Article(Base):
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True)
    title = Column(String, nullable=False, unique=True)
    link = Column(String, nullable=False)
    published = Column(DateTime)
    summary = Column(Text)
    source = Column(String)
    feed_url = Column(String)
    content = Column(Text)  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    author = Column(String)  # –ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏
    category = Column(String)  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
    image_url = Column(String)  # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    word_count = Column(Integer)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    reading_time = Column(Integer)  # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
    is_processed = Column(Boolean, default=False)  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ª–∏ —Å—Ç–∞—Ç—å—è
    created_at = Column(DateTime, default=datetime.now)  # –ö–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ë–î

    def __repr__(self):
        return f"<Article(title='{self.title[:30]}...', source='{self.source}')>"

# --- 3. –§—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---

def setup_database():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∏ —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
    engine = create_engine(DATABASE_URL)
    Base.metadata.create_all(engine) 
    Session = sessionmaker(bind=engine)
    return Session()

def parse_and_save_rss():
    """–ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL, –ø–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é –ª–µ–Ω—Ç—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –ë–î."""
    session = setup_database()
    global_new_count = 0
    
    print(f"üõ†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(RSS_URLS)} RSS-–ª–µ–Ω—Ç...")
    
    for url in RSS_URLS:
        try:
            print(f"üîç –ü–∞—Ä—Å–∏–º –ª–µ–Ω—Ç—É {url}")
            feed = feedparser.parse(url)
            
            if feed.bozo:
                print(f"   ‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: RSS-–ª–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏")
                print(f"   üìã –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {feed.bozo_exception}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–ø–∏—Å–∏ –≤ –ª–µ–Ω—Ç–µ
            if not hasattr(feed, 'entries') or not feed.entries:
                print(f"   ‚ùå –õ–µ–Ω—Ç–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø–∏—Å–µ–π")
                continue
            
            new_count = 0
            feed_title = feed.feed.title if hasattr(feed.feed, 'title') else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫'
            print(f"   üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {feed_title}")
            
            for i, entry in enumerate(feed.entries):
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è
                    exists = session.query(Article).filter_by(title=entry.title).first()
                    if exists:
                        continue
                    
                    print(f"   üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i+1}/{len(feed.entries)}: {entry.title[:50]}...")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata = extract_article_metadata(entry)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
                    print(f"      üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
                    content_result = extract_full_content(entry.link)
                    full_content = content_result.text

                    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    word_count, reading_time = calculate_reading_stats(full_content)

                    content_to_store = full_content
                    if content_result.links:
                        links_block = "\n\n–°—Å—ã–ª–∫–∏:\n" + "\n".join(content_result.links)
                        content_to_store = (full_content + links_block) if full_content else links_block

                    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    new_article = Article(
                        title=entry.title,
                        link=entry.link,
                        published=pub_date,
                        summary=entry.summary if hasattr(entry, 'summary') else '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è',
                        source=feed_title,
                        feed_url=url,
                        content=content_to_store,
                        author=metadata['author'],
                        category=metadata['category'],
                        image_url=metadata['image_url'],
                        word_count=word_count,
                        reading_time=reading_time,
                        is_processed=True
                    )
                    
                    session.add(new_article)
                    new_count += 1
                    global_new_count += 1
                    
                    print(f"      ‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ (—Å–ª–æ–≤: {word_count}, –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {reading_time} –º–∏–Ω)")
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
            
            print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(feed.entries)}, –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {new_count}")
            
        except Exception as e:
            print(f"   - üîß –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –ª–µ–Ω—Ç—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
            continue

    try:
        session.commit()
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        print(f"   –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {global_new_count}")
        print(f"   –§–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {DATABASE_FILE}")
    except Exception as e:
        session.rollback()
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–∫—Å–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏: {e}")
    finally:
        session.close()

# --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–†–û–í–ï–†–ö–ò ---
def check_articles(limit=10):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 'limit' —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î."""
    session = setup_database()
    # –ó–∞–ø—Ä–æ—Å –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ ID (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–Ω–∏–∑—É)
    articles = session.query(Article).order_by(Article.id.desc()).limit(limit).all()
    
    print(f"\n--- –ü–æ—Å–ª–µ–¥–Ω–∏–µ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ({DATABASE_FILE}) ---")
    if not articles:
        print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞.")
        return
    
    for article in articles:
        print("-" * 60)
        print(f"ID: {article.id}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}")
        print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}")
        print(f"–ê–≤—Ç–æ—Ä: {article.author or '–ù–µ —É–∫–∞–∑–∞–Ω'}")
        print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {article.category or '–ù–µ —É–∫–∞–∑–∞–Ω–∞'}")
        print(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {article.published.strftime('%Y-%m-%d %H:%M:%S') if article.published else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}")
        print(f"–î–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {article.created_at.strftime('%Y-%m-%d %H:%M:%S') if article.created_at else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}")
        print(f"–°–ª–æ–≤: {article.word_count or 0}")
        print(f"–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {article.reading_time or 0} –º–∏–Ω")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {'–î–∞' if article.is_processed else '–ù–µ—Ç'}")
        print(f"–°—Å—ã–ª–∫–∞: {article.link}")
        if article.image_url:
            print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {article.image_url}")
        if article.summary:
            print(f"–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {article.summary[:200]}{'...' if len(article.summary) > 200 else ''}")
        if article.content:
            print(f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {article.content[:300]}{'...' if len(article.content) > 300 else ''}")
        
    session.close()

def get_articles_stats():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—å—è–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    session = setup_database()
    
    total_articles = session.query(Article).count()
    processed_articles = session.query(Article).filter(Article.is_processed == True).count()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    sources = session.query(Article.source, session.query(Article).filter(Article.source == Article.source).count()).group_by(Article.source).all()
    
    # –°—Ä–µ–¥–Ω—è—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    avg_words = session.query(Article.word_count).filter(Article.word_count.isnot(None)).all()
    avg_words = sum([w[0] for w in avg_words]) / len(avg_words) if avg_words else 0
    
    print(f"\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ---")
    print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total_articles}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_articles}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {avg_words:.0f}")
    print(f"\n–°—Ç–∞—Ç—å–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    for source, count in sources:
        print(f"  {source}: {count}")
    
    session.close()

# --- –ë–õ–û–ö –ó–ê–ü–£–°–ö–ê ---
if __name__ == "__main__":
    parse_and_save_rss()
    # check_articles(limit=5)  # –í—ã–≤–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    # get_articles_stats()  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
