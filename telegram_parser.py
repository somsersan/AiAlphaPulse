import asyncio
import httpx
import re
import json
import csv
import logging
from bs4 import BeautifulSoup
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean
from sqlalchemy.orm import sessionmaker, declarative_base
from dotenv import load_dotenv
import os

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("parser")

# PostgreSQL –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
load_dotenv()
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://rss_user:rss_password@localhost:5432/rss_db')

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ë–î (SQLAlchemy)
Base = declarative_base()

class Article(Base):
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True)
    title = Column(String, nullable=False, unique=True)
    link = Column(String, nullable=False)
    published = Column(DateTime)
    summary = Column(Text)
    source = Column(String)
    feed_url = Column(String)
    content = Column(Text)  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    author = Column(String)  # –ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏
    category = Column(String)  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
    image_url = Column(String)  # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    word_count = Column(Integer)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    reading_time = Column(Integer)  # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
    is_processed = Column(Boolean, default=False)  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ª–∏ —Å—Ç–∞—Ç—å—è
    created_at = Column(DateTime, default=datetime.now)  # –ö–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ë–î

    def __repr__(self):
        return f"<Article(title='{self.title[:30]}...', source='{self.source}')>"

CHANNELS = [
    'RBCNews', 'vedomosti', 'kommersant', 'tass_agency', 'interfaxonline',
    'bitkogan', 'rbc_quote', 'banksta', 'cbonds', 'investfuture',
    'bezposhady', 'bloomeconomy', 'bloombusiness', 'economistg', 'BizLike',
    'finance_pro_tg', 'banki_economy', 'prbezposhady',
    'ru_holder', 'centralbank_russia', 'AlfaBank', 'multievan', 'bankvtb',
    'ozon_bank_official', 'auantonov', 'prostoecon', 'sberbank', 'cb_economics',
    'MarketOverview', 'frank_media', 'lemonfortea', 'div_invest', 'moneyhack',
    'finside', 'platformaonline', 'sf_education', 'intrinsic_value', 'finamalert',
    'SmartLab', 'tradernet', 'finam_trade', 'rutrade',

    'Bloomberg', 'Investingcom', 'TheEconomist', 'moneycontrolcom',
    'MarketAlert', 'Stocktwits', 'FinvizAlerts', 'CryptoMarkets', 'cbrstocks',
    'beststocks_usadividends', 'if_stocks', 'realvisiontv',
    'spydell_finance', 'WallStreetBets', 'SatoshiCalls', 'Hypercharts',

    'BiggerPockets', 'RyanScribner',
    'JeffRose', 'MarkoWhiteBoardFinance',
    'DevinCarroll', 'BenHedges'
]

KEYWORDS = {
    "–∞–∫—Ü–∏–∑—ã": r"(–∞–∫—Ü–∏–∑|—Ç–∞–±–∞–∫|–∞–ª–∫–æ–≥–æ–ª—å|—Ç–æ–ø–ª–∏–≤–æ|–±–µ–Ω–∑–∏–Ω|—Å–∏–≥–∞—Ä–µ—Ç)",
    "–∞–∫—Ü–∏–∏": r"(–∞–∫—Ü–∏|–±–∏—Ä–∂|—Ç–æ—Ä–≥|–∫–æ—Ç–∏—Ä–æ–≤–∫|–∏–Ω–¥–µ–∫—Å|–º–æ—Å–±–∏—Ä–∂|–º–º–≤–±|ipo|–¥–∏–≤–∏–¥–µ–Ω–¥)",
    "–º–∏—Ä–æ–≤–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞": r"(–¥–æ–ª–ª–∞—Ä|–µ–≤—Ä–æ|–∫—É—Ä—Å|–Ω–µ—Ñ—Ç—å|–∑–æ–ª–æ—Ç–æ|—ç–∫—Å–ø–æ—Ä—Ç|–∏–º–ø–æ—Ä—Ç|—Å–∞–Ω–∫—Ü–∏|–æ–ø–µ–∫|—Ñ—Ä—Å|fed)",
}

BASE_URL = "https://t.me/s/"

@dataclass
class Post:
    channel: str
    post_id: str
    date: str
    text: str
    categories: List[str]
    views: str
    url: str

class TelegramParser:
    def __init__(self):
        self.client = httpx.AsyncClient(
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"},
            follow_redirects=False
        )

    async def fetch_channel(self, channel: str, limit: int = 30) -> List[Post]:
        url = f"{BASE_URL}{channel}"
        posts: List[Post] = []

        try:
            resp = await self.client.get(url, timeout=15)
            if resp.status_code == 302:
                log.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º @{channel} ‚Üí —Ä–µ–¥–∏—Ä–µ–∫—Ç")
                return []

            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")
            messages = soup.find_all("div", class_="tgme_widget_message", limit=limit)

            for msg in messages:
                post = self.parse_message(msg, channel)
                if post:
                    posts.append(post)

            log.info(f"‚úì @{channel}: {len(posts)} –ø–æ—Å—Ç–æ–≤")
        except Exception as e:
            log.error(f"‚úó –û—à–∏–±–∫–∞ @{channel}: {e}")

        return posts

    def parse_message(self, msg, channel: str) -> Post | None:
        try:
            text_div = msg.find("div", class_="tgme_widget_message_text")
            text = text_div.get_text(strip=True) if text_div else ""
            if len(text) < 10:
                return None

            time_elem = msg.find("time")
            date_str = time_elem["datetime"] if time_elem and time_elem.has_attr("datetime") else datetime.now().isoformat()

            link = msg.get("data-post", "")
            post_id = link.split("/")[-1] if link else "unknown"

            views_elem = msg.find("span", class_="tgme_widget_message_views")
            views = views_elem.get_text(strip=True) if views_elem else "0"

            categories = self.categorize(text)


            return Post(
                channel=f"@{channel}",
                post_id=post_id,
                date=date_str,
                text=text.strip(),
                categories=categories,
                views=views,
                url=f"https://t.me/{channel}/{post_id}"
            )
        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ—Å—Ç–∞ @{channel}: {e}")
            return None

    def categorize(self, text: str) -> List[str]:
        text_lower = text.lower()
        categories = [cat for cat, pattern in KEYWORDS.items() if re.search(pattern, text_lower)]
        categories.append("–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏")
        return categories

    async def parse_all(self, channels: List[str], limit: int = 30) -> List[Post]:
        tasks = [self.fetch_channel(ch, limit) for ch in channels]
        results = await asyncio.gather(*tasks)
        return [p for channel_posts in results for p in channel_posts]

    async def close(self):
        await self.client.aclose()

def save_json(posts: List[Post], filename="financial_news.json"):
    data = []
    for i, p in enumerate(posts, start=1):
        article = {
            "id": i,
            "title": p.text[:255],
            "link": p.url,
            "published": p.date,
            "summary": p.text[:500],
            "source": p.channel,
            "feed_url": f"https://t.me/{p.channel[1:]}",
            "content": p.text,
            "author": None,
            "category": ", ".join(p.categories),
            "image_url": None,
            "word_count": len(p.text.split()),
            "reading_time": max(1, len(p.text.split()) // 200),
            "is_processed": False,
            "created_at": datetime.now().isoformat()
        }
        data.append(article)

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    log.info(f"JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")

def save_csv(posts: List[Post], filename="financial_news.csv"):
    if not posts:
        return
    with open(filename, "w", encoding="utf-8", newline="") as f:
        fieldnames = [
            "id","title","link","published","summary","source","feed_url","content",
            "author","category","image_url","word_count","reading_time","is_processed","created_at"
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for i, p in enumerate(posts, start=1):
            writer.writerow({
                "id": i,
                "title": p.text[:255],
                "link": p.url,
                "published": p.date,
                "summary": p.text[:500],
                "source": p.channel,
                "feed_url": f"https://t.me/{p.channel[1:]}",
                "content": p.text,
                "author": None,
                "category": ", ".join(p.categories),
                "image_url": None,
                "word_count": len(p.text.split()),
                "reading_time": max(1, len(p.text.split()) // 200),
                "is_processed": False,
                "created_at": datetime.now().isoformat()
            })
    log.info(f"CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")

def print_stats(posts: List[Post]):
    log.info(f"–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
    cats: Dict[str, int] = {}
    chans: Dict[str, int] = {}
    for p in posts:
        for c in p.categories:
            cats[c] = cats.get(c, 0) + 1
        chans[p.channel] = chans.get(p.channel, 0) + 1

    print("\nüìë –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for c, n in sorted(cats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {c}: {n}")

    print("\nüì∫ –ü–æ –∫–∞–Ω–∞–ª–∞–º:")
    for c, n in sorted(chans.items(), key=lambda x: x[1], reverse=True):
        print(f"  {c}: {n}")

def setup_database():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∏ —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
    engine = create_engine(DATABASE_URL)
    Base.metadata.create_all(engine) 
    Session = sessionmaker(bind=engine)
    return Session()

def calculate_reading_stats(content):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á—Ç–µ–Ω–∏—è."""
    if not content:
        return 0, 0
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞)
    words = re.findall(r'\b\w+\b', content.lower())
    word_count = len(words)
    
    # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–æ 200 —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
    reading_time = max(1, word_count // 200)
    
    return word_count, reading_time

async def parse_and_save_telegram():
    """–ü–∞—Ä—Å–∏—Ç Telegram –∫–∞–Ω–∞–ª—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã –≤ –ë–î."""
    session = setup_database()
    global_new_count = 0
    
    log.info(f"üõ†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤...")
    
    parser = TelegramParser()
    try:
        posts = await parser.parse_all(CHANNELS, limit=20)
        
        for post in posts:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è
                exists = session.query(Article).filter_by(title=post.text[:255]).first()
                if exists:
                    continue
                
                log.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å—Ç: {post.text[:50]}...")
                
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
                pub_date = None
                try:
                    pub_date = datetime.fromisoformat(post.date.replace('Z', '+00:00'))
                except:
                    pub_date = datetime.now()
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                word_count, reading_time = calculate_reading_stats(post.text)
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é
                new_article = Article(
                    title=post.text[:255],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    link=post.url,
                    published=pub_date,
                    summary=post.text[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ summary
                    source=post.channel,
                    feed_url=f"https://t.me/{post.channel[1:]}",  # –£–±–∏—Ä–∞–µ–º @
                    content=post.text,
                    author=None,  # –í Telegram –ø–æ—Å—Ç–∞—Ö –æ–±—ã—á–Ω–æ –Ω–µ—Ç –∞–≤—Ç–æ—Ä–∞
                    category=", ".join(post.categories),
                    image_url=None,  # –ü–æ–∫–∞ –Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    word_count=word_count,
                    reading_time=reading_time,
                    is_processed=True
                )
                
                session.add(new_article)
                global_new_count += 1
                
                log.info(f"‚úÖ –ü–æ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω (—Å–ª–æ–≤: {word_count}, –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {reading_time} –º–∏–Ω)")
                
            except Exception as e:
                log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Å—Ç–∞: {e}")
                continue
        
        session.commit()
        log.info(f"‚úÖ Telegram –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {global_new_count}")
        return global_new_count
        
    except Exception as e:
        session.rollback()
        log.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Telegram: {e}")
        return 0
    finally:
        session.close()
        await parser.close()

async def main():
    log.info(" –°—Ç–∞—Ä—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    parser = TelegramParser()
    posts = await parser.parse_all(CHANNELS, limit=20)
    await parser.close()

    if not posts:
        log.error("–ü–æ—Å—Ç—ã –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")
        return

    print_stats(posts)
    save_json(posts)
    save_csv(posts)
