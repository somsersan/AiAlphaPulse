diff --git a/src/parser/rss_parser_local.py b/src/parser/rss_parser_local.py
index 51874fcb49eedc5bae1a6f2f2d27c7acbe6efa72..69339a8e429b0ae592481573aef92ed65de889f9 100644
--- a/src/parser/rss_parser_local.py
+++ b/src/parser/rss_parser_local.py
@@ -1,33 +1,38 @@
+import json
+from dataclasses import dataclass, field
+from datetime import datetime
+import re
+import time
+from typing import Dict, List, Optional
+from urllib.parse import urljoin
+
 import feedparser
 import requests
 from bs4 import BeautifulSoup
 from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean
 from sqlalchemy.orm import sessionmaker, declarative_base
-from datetime import datetime
-import re
-import time
 
 
     # # Investopedia (–í—Å–µ —Å—Ç–∞—Ç—å–∏)
     # 'https://news.google.com/rss?hl=ru&gl=RU&ceid=RU:ru',
     # 'https://www.investopedia.com/rss-feed-4790074',
 RSS_URLS = [
     # ----------------------------------------------------
     # –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–ò–ù–ê–ù–°–û–í–´–ï –ò –ë–ò–ó–ù–ï–°-–ò–ó–î–ê–ù–ò–Ø
     # ----------------------------------------------------
     # 'https://lenta.ru/rss/news',
     # 'https://habr.com/ru/rss/hubs/all/'
     
     # # Bloomberg
     # 'https://feeds.bloomberg.com/markets/news.rss',
     # 'https://feeds.bloomberg.com/business/news.rss',
         
     # # CNBC (Top News)
     # 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
         
         
     # # Business Insider (Top News)
     # 'https://www.businessinsider.com/rss',
 
 
     # "https://smart-lab.ru/news/rss/",
diff --git a/src/parser/rss_parser_local.py b/src/parser/rss_parser_local.py
index 51874fcb49eedc5bae1a6f2f2d27c7acbe6efa72..69339a8e429b0ae592481573aef92ed65de889f9 100644
--- a/src/parser/rss_parser_local.py
+++ b/src/parser/rss_parser_local.py
@@ -60,108 +65,137 @@ RSS_URLS = [
     
     # ----------------------------------------------------
     # –†–û–°–°–ò–ô–°–ö–ò–ï –≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ò–ï RSS-–õ–ï–ù–¢–´ (–ü–†–û–í–ï–†–ï–ù–´)
     # ----------------------------------------------------
     # 'https://www.kommersant.ru/RSS/news.xml',  # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä - —Ä–∞–±–æ—Ç–∞–µ—Ç
 ]
 
 # –ò–º—è —Ñ–∞–π–ª–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite
 DATABASE_FILE = 'rss_articles.db'
 DATABASE_URL = f"sqlite:///{DATABASE_FILE}"
 
 # --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ë–î (SQLAlchemy) ---
 Base = declarative_base()
 
 class Article(Base):
     __tablename__ = 'articles'
     
     id = Column(Integer, primary_key=True)
     title = Column(String, nullable=False, unique=True)
     link = Column(String, nullable=False)
     published = Column(DateTime)
     summary = Column(Text)
     source = Column(String)
     feed_url = Column(String)
     content = Column(Text)  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
+    links_json = Column(Text)  # –°—Å—ã–ª–∫–∏, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑ —Å—Ç–∞—Ç—å–∏
     author = Column(String)  # –ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏
     category = Column(String)  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
     image_url = Column(String)  # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
     word_count = Column(Integer)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
     reading_time = Column(Integer)  # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
     is_processed = Column(Boolean, default=False)  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ª–∏ —Å—Ç–∞—Ç—å—è
     created_at = Column(DateTime, default=datetime.now)  # –ö–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ë–î
 
     def __repr__(self):
         return f"<Article(title='{self.title[:30]}...', source='{self.source}')>"
 
 # --- 3. –§—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---
 
+@dataclass
+class ContentResult:
+    text: Optional[str]
+    links: List[Dict[str, str]] = field(default_factory=list)
+
+
+def _extract_links(element: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
+    links: List[Dict[str, str]] = []
+    if not element:
+        return links
+
+    for anchor in element.find_all('a', href=True):
+        href = anchor['href'].strip()
+        if not href or href.startswith('#'):
+            continue
+
+        full_href = urljoin(base_url, href)
+        text = anchor.get_text(separator=' ', strip=True) or full_href
+        links.append({'href': full_href, 'text': text})
+
+    return links
+
+
 def extract_full_content(article_url, max_retries=3):
-    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL."""
+    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏ —Å—Å—ã–ª–∫–∏ –ø–æ URL."""
     for attempt in range(max_retries):
         try:
             headers = {
                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
             }
             response = requests.get(article_url, headers=headers, timeout=10)
             response.raise_for_status()
-            
+
             soup = BeautifulSoup(response.content, 'html.parser')
-            
+
             # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
             for script in soup(["script", "style", "nav", "footer", "aside"]):
                 script.decompose()
-            
+
             # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
             content_selectors = [
                 'article', '.article-content', '.post-content', '.entry-content',
                 '.content', '.main-content', '.story-content', '.news-content',
                 '[role="main"]', '.article-body', '.post-body'
             ]
-            
-            content = None
+
+            content_text: Optional[str] = None
+            links: List[Dict[str, str]] = []
             for selector in content_selectors:
                 content_elem = soup.select_one(selector)
                 if content_elem:
-                    content = content_elem.get_text(strip=True)
+                    content_text = content_elem.get_text(separator=' ', strip=True)
+                    links = _extract_links(content_elem, article_url)
                     break
-            
-            if not content:
+
+            if not content_text:
                 # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º body
                 body = soup.find('body')
                 if body:
-                    content = body.get_text(strip=True)
-            
-            return content[:5000] if content else None  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
-            
+                    content_text = body.get_text(separator=' ', strip=True)
+                    links = _extract_links(body, article_url)
+
+            if content_text:
+                return ContentResult(text=content_text[:5000], links=links)
+            return ContentResult(text=None, links=links)
+
         except Exception as e:
             print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
             if attempt < max_retries - 1:
                 time.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
             continue
-    
-    return None
+
+    return ContentResult(text=None, links=[])
 
 def extract_article_metadata(entry):
     """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ RSS-–∑–∞–ø–∏—Å–∏."""
     metadata = {
         'author': None,
         'category': None,
         'image_url': None
     }
     
     # –ê–≤—Ç–æ—Ä
     if hasattr(entry, 'author'):
         metadata['author'] = entry.author
     elif hasattr(entry, 'author_detail') and hasattr(entry.author_detail, 'name'):
         metadata['author'] = entry.author_detail.name
     
     # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ç–µ–≥–∏
     if hasattr(entry, 'tags') and entry.tags:
         categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]
         metadata['category'] = ', '.join(categories[:3])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ç–µ–≥–∞
     
     # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
     if hasattr(entry, 'media_content') and entry.media_content:
         for media in entry.media_content:
             if hasattr(media, 'type') and 'image' in media.type:
                 metadata['image_url'] = media.url
diff --git a/src/parser/rss_parser_local.py b/src/parser/rss_parser_local.py
index 51874fcb49eedc5bae1a6f2f2d27c7acbe6efa72..69339a8e429b0ae592481573aef92ed65de889f9 100644
--- a/src/parser/rss_parser_local.py
+++ b/src/parser/rss_parser_local.py
@@ -219,64 +253,66 @@ def parse_and_save_rss():
                 continue
             
             new_count = 0
             feed_title = feed.feed.title if hasattr(feed.feed, 'title') else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫'
             print(f"   üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {feed_title}")
             
             for i, entry in enumerate(feed.entries):
                 try:
                     # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è
                     exists = session.query(Article).filter_by(title=entry.title).first()
                     if exists:
                         continue
                     
                     print(f"   üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i+1}/{len(feed.entries)}: {entry.title[:50]}...")
                     
                     # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                     pub_date = None
                     if hasattr(entry, 'published_parsed') and entry.published_parsed:
                         pub_date = datetime(*entry.published_parsed[:6])
                     
                     # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                     metadata = extract_article_metadata(entry)
                     
                     # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
                     print(f"      üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
-                    full_content = extract_full_content(entry.link)
-                    
+                    content_result = extract_full_content(entry.link)
+                    full_content = content_result.text
+
                     # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                     word_count, reading_time = calculate_reading_stats(full_content)
                     
                     # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                     new_article = Article(
                         title=entry.title,
                         link=entry.link,
                         published=pub_date,
                         summary=entry.summary if hasattr(entry, 'summary') else '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è',
                         source=feed_title,
                         feed_url=url,
                         content=full_content,
+                        links_json=json.dumps(content_result.links, ensure_ascii=False) if content_result.links else None,
                         author=metadata['author'],
                         category=metadata['category'],
                         image_url=metadata['image_url'],
                         word_count=word_count,
                         reading_time=reading_time,
                         is_processed=True
                     )
                     
                     session.add(new_article)
                     new_count += 1
                     global_new_count += 1
                     
                     print(f"      ‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ (—Å–ª–æ–≤: {word_count}, –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {reading_time} –º–∏–Ω)")
                     
                     # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                     time.sleep(1)
                     
                 except Exception as e:
                     print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
                     continue
             
             print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(feed.entries)}, –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {new_count}")
             
         except Exception as e:
             print(f"   - üîß –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –ª–µ–Ω—Ç—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
diff --git a/src/parser/rss_parser_local.py b/src/parser/rss_parser_local.py
index 51874fcb49eedc5bae1a6f2f2d27c7acbe6efa72..69339a8e429b0ae592481573aef92ed65de889f9 100644
--- a/src/parser/rss_parser_local.py
+++ b/src/parser/rss_parser_local.py
@@ -296,50 +332,62 @@ def parse_and_save_rss():
 # --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–†–û–í–ï–†–ö–ò ---
 def check_articles(limit=10):
     """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 'limit' —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î."""
     session = setup_database()
     # –ó–∞–ø—Ä–æ—Å –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ ID (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–Ω–∏–∑—É)
     articles = session.query(Article).order_by(Article.id.desc()).limit(limit).all()
     
     print(f"\n--- –ü–æ—Å–ª–µ–¥–Ω–∏–µ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ({DATABASE_FILE}) ---")
     if not articles:
         print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞.")
         return
     
     for article in articles:
         print("-" * 60)
         print(f"ID: {article.id}")
         print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}")
         print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}")
         print(f"–ê–≤—Ç–æ—Ä: {article.author or '–ù–µ —É–∫–∞–∑–∞–Ω'}")
         print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {article.category or '–ù–µ —É–∫–∞–∑–∞–Ω–∞'}")
         print(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {article.published.strftime('%Y-%m-%d %H:%M:%S') if article.published else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}")
         print(f"–î–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {article.created_at.strftime('%Y-%m-%d %H:%M:%S') if article.created_at else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}")
         print(f"–°–ª–æ–≤: {article.word_count or 0}")
         print(f"–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {article.reading_time or 0} –º–∏–Ω")
         print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {'–î–∞' if article.is_processed else '–ù–µ—Ç'}")
         print(f"–°—Å—ã–ª–∫–∞: {article.link}")
+        links = []
+        if article.links_json:
+            try:
+                links = json.loads(article.links_json)
+            except json.JSONDecodeError:
+                links = []
+        if links:
+            print("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏:")
+            for link_info in links[:5]:
+                href = link_info.get('href')
+                text = link_info.get('text')
+                print(f"  - {text or href}: {href}")
         if article.image_url:
             print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {article.image_url}")
         if article.summary:
             print(f"–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {article.summary[:200]}{'...' if len(article.summary) > 200 else ''}")
         if article.content:
             print(f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {article.content[:300]}{'...' if len(article.content) > 300 else ''}")
         
     session.close()
 
 def get_articles_stats():
     """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—å—è–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
     session = setup_database()
     
     total_articles = session.query(Article).count()
     processed_articles = session.query(Article).filter(Article.is_processed == True).count()
     
     # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
     sources = session.query(Article.source, session.query(Article).filter(Article.source == Article.source).count()).group_by(Article.source).all()
     
     # –°—Ä–µ–¥–Ω—è—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
     avg_words = session.query(Article.word_count).filter(Article.word_count.isnot(None)).all()
     avg_words = sum([w[0] for w in avg_words]) / len(avg_words) if avg_words else 0
     
     print(f"\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ---")
     print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total_articles}")
